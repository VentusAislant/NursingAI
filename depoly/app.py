import os
import sys
import warnings

# å¿½ç•¥Pydanticç›¸å…³è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="pydantic")

# è®¾ç½®ç¯å¢ƒå˜é‡é¿å…æŸäº›å…¼å®¹æ€§é—®é¢˜
os.environ["GRADIO_ANALYTICS_ENABLED"] = "False"
os.environ["GRADIO_SERVER_NAME"] = "127.0.0.1"

try:
    import gradio as gr
except ImportError as e:
    print(f"âŒ Gradioå¯¼å…¥å¤±è´¥: {e}")
    print("è¯·è¿è¡Œ: pip install gradio==4.16.0")
    sys.exit(1)

try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import PeftModel
except ImportError as e:
    print(f"âŒ æ¨¡å‹ç›¸å…³åº“å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·è¿è¡Œ: pip install torch transformers peft")
    sys.exit(1)

try:
    from xtuner_chat import xtuner_chat
except ImportError as e:
    try:
        import sys

        sys.path.append(os.path.dirname(__file__))
        from xtuner_chat import xtuner_chat
    except ImportError as e:
        print(f"âŒ XTunerèŠå¤©æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿xtuner_chat.pyæ–‡ä»¶å­˜åœ¨")
        sys.exit(1)

try:
    from config import (
        BASE_MODELS, LORA_MODELS, ROLES, LORA_RANKS,
        MODEL_CONFIG, WEB_CONFIG, CHAT_CONFIG, ROLE_SYSTEM_PROMPTS
    )
except ImportError as e:
    # å°è¯•ä»å½“å‰ç›®å½•å¯¼å…¥
    try:
        import sys

        sys.path.append(os.path.dirname(__file__))
        from config import (
            BASE_MODELS, LORA_MODELS, ROLES, LORA_RANKS,
            MODEL_CONFIG, WEB_CONFIG, CHAT_CONFIG, ROLE_SYSTEM_PROMPTS
        )
    except ImportError as e2:
        print(f"âŒ é…ç½®æ–‡ä»¶å¯¼å…¥å¤±è´¥: {e2}")
        sys.exit(1)


class ModelManager:
    def __init__(self):
        self.base_models = BASE_MODELS
        self.lora_models = LORA_MODELS
        self.roles = list(ROLES.keys())
        self.ranks = list(LORA_RANKS.keys())

        self.current_model = None
        self.current_tokenizer = None
        self.current_role = None
        self.current_lora_model = None
        self.current_base_model = None

    def get_available_models(self):
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        return list(self.base_models.keys())

    def get_lora_models_for_base(self, base_model):
        """è·å–æŒ‡å®šbaseæ¨¡å‹å¯¹åº”çš„loraæ¨¡å‹"""
        if base_model in self.lora_models:
            return list(self.lora_models[base_model].keys())
        return []

    def get_roles(self):
        """è·å–å¯ç”¨çš„è§’è‰²åˆ—è¡¨"""
        return self.roles

    def get_ranks(self):
        """è·å–å¯ç”¨çš„rankåˆ—è¡¨"""
        return self.ranks

    def get_iters(self, base_model, lora_model, role, rank):
        """åŠ¨æ€è·å–æŒ‡å®šè·¯å¾„ä¸‹çš„iteråˆ—è¡¨"""
        try:
            # å‚æ•°éªŒè¯
            if not all([base_model, lora_model, role, rank]):
                print(f"å‚æ•°ä¸å®Œæ•´: base_model={base_model}, lora_model={lora_model}, role={role}, rank={rank}")
                return []

            # æ£€æŸ¥base_modelæ˜¯å¦å­˜åœ¨
            if base_model not in self.lora_models:
                print(f"base_modelä¸å­˜åœ¨: {base_model}")
                return []

            # æ£€æŸ¥lora_modelæ˜¯å¦å­˜åœ¨
            if lora_model not in self.lora_models[base_model]:
                print(f"lora_modelä¸å­˜åœ¨: {lora_model} in {base_model}")
                return []

            # æ„å»ºè·¯å¾„
            lora_path = os.path.join(
                self.lora_models[base_model][lora_model]["path"],
                role,
                rank
            )

            if not os.path.exists(lora_path):
                print(f"è·¯å¾„ä¸å­˜åœ¨: {lora_path}")
                return []

            # è·å–æ‰€æœ‰iter_å¼€å¤´çš„ç›®å½•
            iter_dirs = []
            for item in os.listdir(lora_path):
                if item.startswith("iter_") and os.path.isdir(os.path.join(lora_path, item)):
                    iter_dirs.append(item)

            # æŒ‰è¿­ä»£æ¬¡æ•°æ’åº
            iter_dirs.sort(key=lambda x: int(x.split("_")[1]))
            return iter_dirs

        except Exception as e:
            print(f"è·å–iteråˆ—è¡¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []

    def get_model_info(self, base_model, lora_model, role, rank, iter_name):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        info = {
            "base_model": self.base_models[base_model]["description"] if base_model in self.base_models else "",
            "lora_model": self.lora_models[base_model][lora_model][
                "description"] if base_model in self.lora_models and lora_model in self.lora_models[base_model] else "",
            "role": ROLES[role]["description"] if role in ROLES else "",
            "rank": LORA_RANKS[rank]["description"] if rank in LORA_RANKS else "",
            "iter": f"è¿­ä»£æ¬¡æ•°: {iter_name.split('_')[1]}" if iter_name else ""
        }
        return info

    def load_model(self, base_model, lora_model, role, rank, iter_name):
        """åŠ è½½æŒ‡å®šçš„æ¨¡å‹"""
        try:
            # ä¿å­˜å½“å‰é€‰æ‹©çš„è§’è‰²å’Œæ¨¡å‹ä¿¡æ¯
            self.current_role = role
            self.current_lora_model = lora_model
            self.current_base_model = base_model

            # æ„å»ºloraæ¨¡å‹è·¯å¾„
            lora_path = os.path.join(
                self.lora_models[base_model][lora_model]["path"],
                role,
                rank,
                iter_name
            )

            if not os.path.exists(lora_path):
                return f"é”™è¯¯ï¼šæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ - {lora_path}"

            # åŠ è½½baseæ¨¡å‹
            base_path = self.base_models[base_model]["path"]
            print(f"æ­£åœ¨åŠ è½½baseæ¨¡å‹: {base_path}")

            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ä¸åŒçš„åŠ è½½æ–¹å¼
            model_type = self.base_models[base_model]["type"]

            # æ£€æŸ¥GPUå¯ç”¨æ€§
            if torch.cuda.is_available() and not MODEL_CONFIG["force_cpu"]:
                print(f"ğŸš€ ä½¿ç”¨GPUè¿›è¡Œæ¨ç†")
                device_map = "auto"  # è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡
            else:
                print(f"ğŸ’» ä½¿ç”¨CPUè¿›è¡Œæ¨ç†")
                device_map = "cpu"

            if model_type == "llama":
                self.current_tokenizer = AutoTokenizer.from_pretrained(
                    base_path,
                    trust_remote_code=MODEL_CONFIG["trust_remote_code"]
                )
                self.current_model = AutoModelForCausalLM.from_pretrained(
                    base_path,
                    torch_dtype=getattr(torch, MODEL_CONFIG["torch_dtype"]),
                    device_map=device_map,
                    trust_remote_code=MODEL_CONFIG["trust_remote_code"],
                    low_cpu_mem_usage=MODEL_CONFIG["low_cpu_mem_usage"]
                )
            else:
                self.current_tokenizer = AutoTokenizer.from_pretrained(
                    base_path,
                    trust_remote_code=MODEL_CONFIG["trust_remote_code"]
                )
                self.current_model = AutoModelForCausalLM.from_pretrained(
                    base_path,
                    torch_dtype=getattr(torch, MODEL_CONFIG["torch_dtype"]),
                    device_map=device_map,
                    trust_remote_code=MODEL_CONFIG["trust_remote_code"],
                    low_cpu_mem_usage=MODEL_CONFIG["low_cpu_mem_usage"]
                )

            # æ£€æŸ¥æ¨¡å‹è®¾å¤‡åˆ†é…
            if torch.cuda.is_available() and not MODEL_CONFIG["force_cpu"]:
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨GPUä¸Š
                model_devices = set()
                for param in self.current_model.parameters():
                    model_devices.add(param.device)

                print(f"ğŸ“Š æ¨¡å‹è®¾å¤‡åˆ†å¸ƒ: {list(model_devices)}")

                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†Accelerate offload
                has_meta_device = any('meta' in str(device) for device in model_devices)
                has_cpu_device = any('cpu' in str(device) for device in model_devices)

                if has_meta_device:
                    print(f"â„¹ï¸  æ¨¡å‹ä½¿ç”¨äº†Accelerate offloadï¼Œä¿æŒå½“å‰è®¾å¤‡åˆ†å¸ƒ")
                elif has_cpu_device and len(model_devices) > 1:
                    print(f"âš ï¸  æ£€æµ‹åˆ°æ¨¡å‹éƒ¨åˆ†åœ¨CPUï¼Œå°è¯•ç§»åŠ¨åˆ°GPU...")
                    try:
                        # æ¸…ç†GPUå†…å­˜
                        torch.cuda.empty_cache()
                        self.current_model = self.current_model.to('cuda:0')
                        # éªŒè¯ç§»åŠ¨ç»“æœ
                        new_devices = set()
                        for param in self.current_model.parameters():
                            new_devices.add(param.device)
                        print(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°GPU: {list(new_devices)}")
                    except Exception as e:
                        print(f"âŒ ç§»åŠ¨æ¨¡å‹åˆ°GPUå¤±è´¥: {e}")
                        print(f"â„¹ï¸  ä¿æŒå½“å‰è®¾å¤‡åˆ†å¸ƒ")
                else:
                    print(f"âœ… æ¨¡å‹è®¾å¤‡åˆ†å¸ƒæ­£å¸¸")
            else:
                print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ°CPU")

            # åŠ è½½loraæ¨¡å‹
            print(f"æ­£åœ¨åŠ è½½loraæ¨¡å‹: {lora_path}")
            self.current_model = PeftModel.from_pretrained(
                self.current_model,
                lora_path,
                torch_dtype=getattr(torch, MODEL_CONFIG["torch_dtype"]),
                is_trainable=False  # æ¨ç†æ¨¡å¼ï¼ŒèŠ‚çœå†…å­˜
            )

            # éªŒè¯LoRAæ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½
            print(f"âœ… LoRAæ¨¡å‹åŠ è½½å®Œæˆ")
            print(f"ğŸ“Š æ¨¡å‹ç±»å‹: {type(self.current_model)}")
            if hasattr(self.current_model, 'peft_config'):
                print(f"ğŸ“‹ LoRAé…ç½®: {list(self.current_model.peft_config.keys())}")

            # æ£€æŸ¥LoRAæ¨¡å‹è®¾å¤‡åˆ†é…
            if torch.cuda.is_available() and not MODEL_CONFIG["force_cpu"]:
                lora_devices = set()
                for param in self.current_model.parameters():
                    lora_devices.add(param.device)

                print(f"ğŸ“Š LoRAæ¨¡å‹è®¾å¤‡åˆ†å¸ƒ: {list(lora_devices)}")

                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†Accelerate offload
                has_meta_device = any('meta' in str(device) for device in lora_devices)
                has_cpu_device = any('cpu' in str(device) for device in lora_devices)

                if has_meta_device:
                    print(f"â„¹ï¸  LoRAæ¨¡å‹ä½¿ç”¨äº†Accelerate offloadï¼Œä¿æŒå½“å‰è®¾å¤‡åˆ†å¸ƒ")
                elif has_cpu_device:
                    print(f"âš ï¸  æ£€æµ‹åˆ°LoRAæ¨¡å‹åœ¨CPUï¼Œå°è¯•ç§»åŠ¨åˆ°GPU...")
                    try:
                        # æ¸…ç†GPUå†…å­˜
                        torch.cuda.empty_cache()
                        self.current_model = self.current_model.to('cuda:0')
                        # éªŒè¯ç§»åŠ¨ç»“æœ
                        new_devices = set()
                        for param in self.current_model.parameters():
                            new_devices.add(param.device)
                        print(f"âœ… LoRAæ¨¡å‹å·²ç§»åŠ¨åˆ°GPU: {list(new_devices)}")
                    except Exception as e:
                        print(f"âŒ ç§»åŠ¨LoRAæ¨¡å‹åˆ°GPUå¤±è´¥: {e}")
                        print(f"â„¹ï¸  ä¿æŒå½“å‰è®¾å¤‡åˆ†å¸ƒï¼Œä½¿ç”¨CPUæ¨ç†")
                else:
                    print(f"âœ… LoRAæ¨¡å‹è®¾å¤‡åˆ†å¸ƒæ­£å¸¸")
            else:
                print(f"âœ… LoRAæ¨¡å‹å·²åŠ è½½åˆ°CPU")

            # æ‰“å°æ¨¡å‹è¯¦ç»†ä¿¡æ¯
            print(f"\nğŸ” æ¨¡å‹è¯¦ç»†ä¿¡æ¯:")
            print(f"   Baseæ¨¡å‹: {base_model}")
            print(f"   LoRAæ¨¡å‹: {lora_model}")
            print(f"   è§’è‰²: {role}")
            print(f"   Rank: {rank}")
            print(f"   Iter: {iter_name}")
            print(f"   æ¨¡å‹ç±»å‹: {model_type}")

            # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
            if torch.cuda.is_available():
                print(f"   ğŸš€ GPUè®¾å¤‡: {torch.cuda.get_device_name()}")
                total_memory = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3
                allocated_memory = torch.cuda.memory_allocated() / 1024 ** 3
                reserved_memory = torch.cuda.memory_reserved() / 1024 ** 3
                free_memory = total_memory - reserved_memory

                print(f"   ğŸ“Š GPUæ€»å†…å­˜: {total_memory:.1f}GB")
                print(f"   ğŸ’¾ å·²ç”¨å†…å­˜: {allocated_memory:.1f}GB")
                print(f"   ğŸ”„ ç¼“å­˜å†…å­˜: {reserved_memory:.1f}GB")
                print(f"   ğŸ†“ å¯ç”¨å†…å­˜: {free_memory:.1f}GB")

                # å†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯”
                memory_usage = (reserved_memory / total_memory) * 100
                print(f"   ğŸ“ˆ å†…å­˜ä½¿ç”¨ç‡: {memory_usage:.1f}%")
            else:
                print(f"   ğŸ’» ä½¿ç”¨CPUæ¨¡å¼")

            # æ£€æŸ¥æ¨¡å‹å®é™…è®¾å¤‡åˆ†å¸ƒ
            model_devices = set()
            for param in self.current_model.parameters():
                model_devices.add(str(param.device))

            print(f"   æ¨¡å‹è®¾å¤‡åˆ†å¸ƒ: {list(model_devices)}")
            print(f"   å‚æ•°æ•°é‡: {sum(p.numel() for p in self.current_model.parameters()):,}")

            # è·å–æ¨¡å‹ä¿¡æ¯
            model_info = self.get_model_info(base_model, lora_model, role, rank, iter_name)

            # è·å–å½“å‰ç³»ç»Ÿæç¤º
            current_system_prompt = self._get_system_prompt()

            return f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼\n\nğŸ“‹ æ¨¡å‹ä¿¡æ¯ï¼š\nâ€¢ Baseæ¨¡å‹: {base_model}\n  {model_info['base_model']}\nâ€¢ Loraæ¨¡å‹: {lora_model}\n  {model_info['lora_model']}\nâ€¢ è§’è‰²: {ROLES[role]['name']}\n  {model_info['role']}\nâ€¢ Rank: {rank}\n  {model_info['rank']}\nâ€¢ Iter: {iter_name}\n  {model_info['iter']}", current_system_prompt

        except Exception as e:
            return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"

    def chat(self, message, history):
        """è¿›è¡ŒèŠå¤©å¯¹è¯ - ä½¿ç”¨XTunerè¿›è¡Œå¯¹è¯ç”Ÿæˆ"""
        if self.current_model is None or self.current_tokenizer is None:
            return CHAT_CONFIG["default_response"]

        try:
            # è·å–å½“å‰æ¨¡å‹ç±»å‹
            model_type = self._get_model_type()

            # æ ¹æ®è§’è‰²å’Œæ¨¡å‹ç±»å‹é€‰æ‹©system prompt
            system_prompt = self._get_system_prompt()

            # ä½¿ç”¨XTunerè¿›è¡Œå¯¹è¯ç”Ÿæˆ
            response = xtuner_chat.generate_response(
                model=self.current_model,
                tokenizer=self.current_tokenizer,
                message=message,
                history=history,
                system_prompt=system_prompt,
                model_type=model_type,
                max_new_tokens=MODEL_CONFIG["max_new_tokens"],
                temperature=MODEL_CONFIG["temperature"],
                do_sample=MODEL_CONFIG["do_sample"]
            )

            return response

        except Exception as e:
            print(f"âŒ å¯¹è¯ç”Ÿæˆå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return f"âŒ å¯¹è¯ç”Ÿæˆå¤±è´¥: {str(e)}"

    def chat_stream(self, message, history):
        """è¿›è¡Œæµå¼èŠå¤©å¯¹è¯ - ä½¿ç”¨XTunerè¿›è¡Œæµå¼å¯¹è¯ç”Ÿæˆ"""
        if self.current_model is None or self.current_tokenizer is None:
            yield CHAT_CONFIG["default_response"]
            return

        try:
            # è·å–å½“å‰æ¨¡å‹ç±»å‹
            model_type = self._get_model_type()

            # æ ¹æ®è§’è‰²å’Œæ¨¡å‹ç±»å‹é€‰æ‹©system prompt
            system_prompt = self._get_system_prompt()

            # ä½¿ç”¨XTunerè¿›è¡Œæµå¼å¯¹è¯ç”Ÿæˆ
            for response in xtuner_chat.generate_response_stream(
                    model=self.current_model,
                    tokenizer=self.current_tokenizer,
                    message=message,
                    history=history,
                    system_prompt=system_prompt,
                    model_type=model_type,
                    max_new_tokens=MODEL_CONFIG["max_new_tokens"],
                    temperature=MODEL_CONFIG["temperature"],
                    do_sample=MODEL_CONFIG["do_sample"]
            ):
                yield response

        except Exception as e:
            print(f"âŒ æµå¼å¯¹è¯ç”Ÿæˆå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            yield f"âŒ æµå¼å¯¹è¯ç”Ÿæˆå¤±è´¥: {str(e)}"

    def _get_model_type(self):
        """è·å–å½“å‰æ¨¡å‹ç±»å‹"""
        if self.current_base_model is None:
            return "llama"

        model_type = self.base_models[self.current_base_model]["type"]
        return model_type

    def _get_system_prompt(self):
        """æ ¹æ®è§’è‰²å’Œæ¨¡å‹ç±»å‹è·å–system prompt"""
        if self.current_role is None or self.current_lora_model is None:
            return CHAT_CONFIG["system_prompt"]

        # æ£€æŸ¥æ˜¯å¦æ˜¯ft_ptç‰ˆæœ¬ï¼ˆéœ€è¦ç³»ç»Ÿæç¤ºè¯ï¼‰
        if self.current_lora_model.endswith("_ft_pt"):
            # æ ¹æ®è§’è‰²è¿”å›å¯¹åº”çš„system prompt
            if self.current_role in ROLE_SYSTEM_PROMPTS:
                return ROLE_SYSTEM_PROMPTS[self.current_role]
            else:
                return CHAT_CONFIG["system_prompt"]
        else:
            # ftç‰ˆæœ¬ç›´æ¥è¿”å›ç©ºå­—ç¬¦ä¸²
            return ""


# åˆ›å»ºæ¨¡å‹ç®¡ç†å™¨å®ä¾‹
model_manager = ModelManager()


def update_lora_models(base_model):
    """æ›´æ–°loraæ¨¡å‹é€‰é¡¹"""
    if not base_model:
        return gr.Dropdown(choices=[], value=None)

    choices = model_manager.get_lora_models_for_base(base_model)
    return gr.Dropdown(choices=choices, value=choices[0] if choices else None)


def update_iters(base_model, lora_model, role, rank):
    """æ›´æ–°iteré€‰é¡¹"""
    if not all([base_model, lora_model, role, rank]):
        return gr.Dropdown(choices=[], value=None)

    choices = model_manager.get_iters(base_model, lora_model, role, rank)
    return gr.Dropdown(choices=choices, value=choices[0] if choices else None)


def load_model_wrapper(base_model, lora_model, role, rank, iter_name):
    """åŠ è½½æ¨¡å‹çš„åŒ…è£…å‡½æ•°"""
    result = model_manager.load_model(base_model, lora_model, role, rank, iter_name)
    if isinstance(result, tuple):
        return result
    else:
        return result, ""


def chat_wrapper(message, history):
    """èŠå¤©çš„åŒ…è£…å‡½æ•°ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰"""
    response = model_manager.chat(message, history)
    # Gradio ChatbotæœŸæœ›å…ƒç»„æ ¼å¼ (user_message, assistant_message)
    return history + [[message, response]]


def chat_stream_wrapper(message, history):
    """æµå¼èŠå¤©çš„åŒ…è£…å‡½æ•°"""
    # å…ˆæ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    history.append([message, ""])

    # æµå¼ç”Ÿæˆå›å¤
    for response in model_manager.chat_stream(message, history[:-1]):  # ä¼ å…¥é™¤æœ€åä¸€æ¡å¤–çš„å†å²
        # åªæ›´æ–°åŠ©æ‰‹çš„å›å¤éƒ¨åˆ†ï¼Œä¸é‡å¤ç”¨æˆ·è¾“å…¥
        history[-1][1] = response
        yield history


# åˆ›å»ºGradioç•Œé¢
with gr.Blocks(title=WEB_CONFIG["title"], theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¥ NursingAI æ™ºèƒ½èŠå¤©ç³»ç»Ÿ")
    gr.Markdown("è¯·é€‰æ‹©baseæ¨¡å‹å’Œå¯¹åº”çš„loraæ¨¡å‹ï¼Œç„¶åå¼€å§‹èŠå¤©å¯¹è¯")

    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### æ¨¡å‹é€‰æ‹©")

            # Baseæ¨¡å‹é€‰æ‹©
            base_model_dropdown = gr.Dropdown(
                choices=model_manager.get_available_models(),
                label="é€‰æ‹©Baseæ¨¡å‹",
                value=model_manager.get_available_models()[0] if model_manager.get_available_models() else None
            )

            # Loraæ¨¡å‹é€‰æ‹©
            initial_base_model = model_manager.get_available_models()[
                0] if model_manager.get_available_models() else None
            initial_lora_models = model_manager.get_lora_models_for_base(
                initial_base_model) if initial_base_model else []
            initial_lora_model = initial_lora_models[0] if initial_lora_models else None

            lora_model_dropdown = gr.Dropdown(
                choices=initial_lora_models,
                label="é€‰æ‹©Loraæ¨¡å‹",
                interactive=True,
                value=initial_lora_model
            )

            # è§’è‰²é€‰æ‹©
            role_dropdown = gr.Dropdown(
                choices=model_manager.get_roles(),
                label="é€‰æ‹©è§’è‰²",
                value=model_manager.get_roles()[0] if model_manager.get_roles() else None
            )

            # Ranké€‰æ‹©
            rank_dropdown = gr.Dropdown(
                choices=model_manager.get_ranks(),
                label="é€‰æ‹©Lora Rank",
                value=model_manager.get_ranks()[0] if model_manager.get_ranks() else None
            )

            # Iteré€‰æ‹©
            initial_roles = model_manager.get_roles()
            initial_ranks = model_manager.get_ranks()
            initial_role = initial_roles[0] if initial_roles else None
            initial_rank = initial_ranks[0] if initial_ranks else None

            initial_iters = model_manager.get_iters(initial_base_model, initial_lora_model, initial_role,
                                                    initial_rank) if all(
                [initial_base_model, initial_lora_model, initial_role, initial_rank]) else []
            initial_iter = initial_iters[0] if initial_iters else None

            iter_dropdown = gr.Dropdown(
                choices=initial_iters,
                label="é€‰æ‹©Iter",
                interactive=True,
                value=initial_iter
            )

            # åŠ è½½æ¨¡å‹æŒ‰é’®
            load_btn = gr.Button("ğŸš€ åŠ è½½æ¨¡å‹", variant="primary")

            # åŠ è½½çŠ¶æ€æ˜¾ç¤º
            load_status = gr.Textbox(
                label="åŠ è½½çŠ¶æ€",
                interactive=False,
                lines=4
            )

        with gr.Column(scale=2):
            gr.Markdown("### èŠå¤©å¯¹è¯")

            # å½“å‰ç³»ç»Ÿæç¤ºæ˜¾ç¤º
            system_prompt_display = gr.Textbox(
                label="å½“å‰ç³»ç»Ÿæç¤º",
                interactive=False,
                lines=3,
                max_lines=5,
                placeholder="åŠ è½½æ¨¡å‹åæ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„ç³»ç»Ÿæç¤º..."
            )

            # èŠå¤©ç•Œé¢
            chatbot = gr.Chatbot(
                label="èŠå¤©è®°å½•",
                height=500,
                show_label=True,
                type="messages"  # ä½¿ç”¨æ–°çš„æ¶ˆæ¯æ ¼å¼
            )

            # è¾“å…¥æ¡†
            msg = gr.Textbox(
                label="è¾“å…¥æ¶ˆæ¯",
                placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                lines=2
            )

            # å‘é€æŒ‰é’®
            send_btn = gr.Button("ğŸ’¬ å‘é€", variant="primary")

            # æ¸…é™¤æŒ‰é’®
            clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯")

    # ç»‘å®šäº‹ä»¶
    base_model_dropdown.change(
        fn=update_lora_models,
        inputs=[base_model_dropdown],
        outputs=[lora_model_dropdown]
    ).then(
        fn=update_iters,
        inputs=[base_model_dropdown, lora_model_dropdown, role_dropdown, rank_dropdown],
        outputs=[iter_dropdown]
    )

    lora_model_dropdown.change(
        fn=update_iters,
        inputs=[base_model_dropdown, lora_model_dropdown, role_dropdown, rank_dropdown],
        outputs=[iter_dropdown]
    )

    role_dropdown.change(
        fn=update_iters,
        inputs=[base_model_dropdown, lora_model_dropdown, role_dropdown, rank_dropdown],
        outputs=[iter_dropdown]
    )

    rank_dropdown.change(
        fn=update_iters,
        inputs=[base_model_dropdown, lora_model_dropdown, role_dropdown, rank_dropdown],
        outputs=[iter_dropdown]
    )

    load_btn.click(
        fn=load_model_wrapper,
        inputs=[base_model_dropdown, lora_model_dropdown, role_dropdown, rank_dropdown, iter_dropdown],
        outputs=[load_status, system_prompt_display]
    )

    send_btn.click(
        fn=chat_stream_wrapper,
        inputs=[msg, chatbot],
        outputs=[chatbot]
    ).then(
        lambda: "",
        None,
        [msg]
    )

    msg.submit(
        fn=chat_stream_wrapper,
        inputs=[msg, chatbot],
        outputs=[chatbot]
    ).then(
        lambda: "",
        None,
        [msg]
    )

    clear_btn.click(
        lambda: [],
        None,
        [chatbot]
    )

if __name__ == "__main__":
    demo.launch(
        server_name=WEB_CONFIG["server_name"],
        server_port=WEB_CONFIG["server_port"],
        share=WEB_CONFIG["share"],
        debug=WEB_CONFIG["debug"],
        show_error=WEB_CONFIG["show_error"]
    )
