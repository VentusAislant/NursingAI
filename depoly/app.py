import os
import sys
import warnings

# å¿½ç•¥Pydanticç›¸å…³è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="pydantic")

# è®¾ç½®ç¯å¢ƒå˜é‡é¿å…æŸäº›å…¼å®¹æ€§é—®é¢˜
os.environ["GRADIO_ANALYTICS_ENABLED"] = "False"
os.environ["GRADIO_SERVER_NAME"] = "127.0.0.1"

try:
    import gradio as gr
except ImportError as e:
    print(f"âŒ Gradioå¯¼å…¥å¤±è´¥: {e}")
    print("è¯·è¿è¡Œ: pip install gradio==4.16.0")
    sys.exit(1)

try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import PeftModel
except ImportError as e:
    print(f"âŒ æ¨¡å‹ç›¸å…³åº“å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·è¿è¡Œ: pip install torch transformers peft")
    sys.exit(1)

try:
    from chat import chat
except ImportError as e:
    try:
        import sys
        sys.path.append(os.path.dirname(__file__))
        from chat import chat
    except ImportError as e:
        print(f"âŒ chatæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿chat.pyæ–‡ä»¶å­˜åœ¨")
        sys.exit(1)

try:
    from config import (
        BASE_MODELS, LORA_MODELS, ROLES, LORA_RANKS,
        MODEL_CONFIG, WEB_CONFIG, CHAT_CONFIG, ROLE_SYSTEM_PROMPTS, ROLE_MAX_TOKENS
    )
except ImportError as e:
    # å°è¯•ä»å½“å‰ç›®å½•å¯¼å…¥
    try:
        import sys
        sys.path.append(os.path.dirname(__file__))
        from config import (
            BASE_MODELS, LORA_MODELS, ROLES, LORA_RANKS,
            MODEL_CONFIG, WEB_CONFIG, CHAT_CONFIG, ROLE_SYSTEM_PROMPTS, ROLE_MAX_TOKENS
        )
    except ImportError as e2:
        print(f"âŒ é…ç½®æ–‡ä»¶å¯¼å…¥å¤±è´¥: {e2}")
        sys.exit(1)

class ModelManager:
    def __init__(self):
        self.base_models = BASE_MODELS
        self.lora_models = LORA_MODELS
        self.roles = list(ROLES.keys())
        self.ranks = list(LORA_RANKS.keys())
        
        self.current_role = None
        self.current_lora_model = None
        self.current_base_model = None
        
    def get_available_models(self):
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        return list(self.base_models.keys())
    
    def get_lora_models_for_base(self, base_model):
        """è·å–æŒ‡å®šbaseæ¨¡å‹å¯¹åº”çš„loraæ¨¡å‹ï¼Œæ”¯æŒNone"""
        if base_model in self.lora_models:
            # è¿”å›æ˜¾ç¤ºåå’Œå®é™…å€¼
            return [("æ— LoRA", None)] + [(k, k) for k in self.lora_models[base_model].keys()]
        return [("æ— LoRA", None)]

    def get_roles(self):
        """è·å–å¯ç”¨çš„è§’è‰²åˆ—è¡¨"""
        return self.roles
    
    def get_ranks(self):
        """è·å–å¯ç”¨çš„rankåˆ—è¡¨"""
        return self.ranks
    
    def get_iters(self, base_model, lora_model, role, rank):
        """åŠ¨æ€è·å–æŒ‡å®šè·¯å¾„ä¸‹çš„iteråˆ—è¡¨"""
        try:
            # å‚æ•°éªŒè¯
            if not all([base_model, role, rank]):
                print(f"å‚æ•°ä¸å®Œæ•´: base_model={base_model}, role={role}, rank={rank}")
                return []
            
            # æ£€æŸ¥base_modelæ˜¯å¦å­˜åœ¨
            if base_model not in self.lora_models:
                print(f"base_modelä¸å­˜åœ¨: {base_model}")
                return []
            
            # æ£€æŸ¥lora_modelæ˜¯å¦å­˜åœ¨
            if lora_model not in self.lora_models[base_model]:
                print(f"lora_modelä¸å­˜åœ¨: {lora_model} in {base_model}")
                return []
            
            # æ„å»ºè·¯å¾„
            lora_path = os.path.join(
                self.lora_models[base_model][lora_model]["path"],
                role,
                rank
            )
            
            if not os.path.exists(lora_path):
                print(f"è·¯å¾„ä¸å­˜åœ¨: {lora_path}")
                return []
            
            # è·å–æ‰€æœ‰iter_å¼€å¤´çš„ç›®å½•
            iter_dirs = []
            for item in os.listdir(lora_path):
                if item.startswith("iter_") and os.path.isdir(os.path.join(lora_path, item)):
                    iter_dirs.append(item)
            
            # æŒ‰è¿­ä»£æ¬¡æ•°æ’åº
            iter_dirs.sort(key=lambda x: int(x.split("_")[1]))
            return iter_dirs
            
        except Exception as e:
            print(f"è·å–iteråˆ—è¡¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def get_model_info(self, base_model, lora_model, role, rank, iter_name):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        # å¤„ç†lora_modelä¸ºNoneçš„æƒ…å†µ
        lora_description = ""
        if lora_model is not None and base_model in self.lora_models and lora_model in self.lora_models[base_model]:
            lora_description = self.lora_models[base_model][lora_model]["description"]
        elif lora_model is None:
            lora_description = "æ— LoRA"
        
        # å¤„ç†rankä¸º"æ— "çš„æƒ…å†µ
        rank_description = ""
        if rank == "æ— ":
            rank_description = "æ— "
        elif rank in LORA_RANKS:
            rank_description = LORA_RANKS[rank]["description"]
        
        info = {
            "base_model": self.base_models[base_model]["description"] if base_model in self.base_models else "",
            "lora_model": lora_description,
            "role": ROLES[role]["description"] if role in ROLES else "",
            "rank": rank_description,
            "iter": f"è¿­ä»£æ¬¡æ•°: {iter_name.split('_')[1]}" if iter_name and iter_name != "æ— " and iter_name.startswith("iter_") else (iter_name if iter_name else "")
        }
        return info
    
    def load_model(self, base_model, lora_model, role, rank, iter_name):
        """åŠ è½½æŒ‡å®šçš„æ¨¡å‹ - ä½¿ç”¨XTunerè¿›è¡Œæ¨¡å‹åŠ è½½"""
        try:
            # ä¿å­˜å½“å‰é€‰æ‹©çš„è§’è‰²å’Œæ¨¡å‹ä¿¡æ¯
            self.current_role = role
            self.current_lora_model = lora_model
            self.current_base_model = base_model
            
            # æ„å»ºloraæ¨¡å‹è·¯å¾„
            if lora_model is None:
                adapter_path = None
            else:
                adapter_path = os.path.join(
                    self.lora_models[base_model][lora_model]["path"],
                    role,
                    rank,
                    iter_name
                )
                if not os.path.exists(adapter_path):
                    return f"é”™è¯¯ï¼šæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ - {adapter_path}"
            
            # åŠ è½½baseæ¨¡å‹
            base_path = self.base_models[base_model]["path"]
            print(f"æ­£åœ¨åŠ è½½baseæ¨¡å‹: {base_path}")
            
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ä¸åŒçš„åŠ è½½æ–¹å¼
            model_type = self.base_models[base_model]["type"]
            
            # ä½¿ç”¨Chatè¿›è¡Œæ¨¡å‹åŠ è½½
            success = chat.load_model(
                model_name_or_path=base_path,
                adapter_path=adapter_path,
                torch_dtype=MODEL_CONFIG["torch_dtype"],
                bits=None  # å¯ä»¥æ ¹æ®éœ€è¦è®¾ç½®é‡åŒ–
            )
            
            if not success:
                return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥"
            
            # éªŒè¯æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½
            if chat.model is None or chat.tokenizer is None:
                return f"âŒ æ¨¡å‹åŠ è½½éªŒè¯å¤±è´¥"
            
            # æ¸…ç©ºå†å²è®°å½•ï¼Œå‡†å¤‡æ–°çš„å¯¹è¯
            chat.clear_history()
            
            # æ‰“å°æ¨¡å‹è¯¦ç»†ä¿¡æ¯
            print(f"\nğŸ” æ¨¡å‹è¯¦ç»†ä¿¡æ¯:")
            print(f"   Baseæ¨¡å‹: {base_model}")
            print(f"   LoRAæ¨¡å‹: {lora_model if lora_model else 'æ— LoRA'}")
            print(f"   è§’è‰²: {role}")
            print(f"   Rank: {rank}")
            print(f"   Iter: {iter_name}")
            print(f"   æ¨¡å‹ç±»å‹: {model_type}")
            
            # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
            if torch.cuda.is_available():
                print(f"   ğŸš€ GPUè®¾å¤‡: {torch.cuda.get_device_name()}")
                total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                allocated_memory = torch.cuda.memory_allocated() / 1024**3
                reserved_memory = torch.cuda.memory_reserved() / 1024**3
                free_memory = total_memory - reserved_memory
                
                print(f"   ğŸ“Š GPUæ€»å†…å­˜: {total_memory:.1f}GB")
                print(f"   ğŸ’¾ å·²ç”¨å†…å­˜: {allocated_memory:.1f}GB")
                print(f"   ğŸ”„ ç¼“å­˜å†…å­˜: {reserved_memory:.1f}GB")
                print(f"   ğŸ†“ å¯ç”¨å†…å­˜: {free_memory:.1f}GB")
                
                # å†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯”
                memory_usage = (reserved_memory / total_memory) * 100
                print(f"   ğŸ“ˆ å†…å­˜ä½¿ç”¨ç‡: {memory_usage:.1f}%")
                
                # æ£€æŸ¥æ¨¡å‹è®¾å¤‡
                if hasattr(chat.model, 'device'):
                    print(f"   ğŸ¯ æ¨¡å‹è®¾å¤‡: {chat.model.device}")
                else:
                    model_device = next(chat.model.parameters()).device
                    print(f"   ğŸ¯ æ¨¡å‹è®¾å¤‡: {model_device}")
            else:
                print(f"   ğŸ’» ä½¿ç”¨CPUæ¨¡å¼")
            
            # è·å–æ¨¡å‹ä¿¡æ¯
            model_info = self.get_model_info(base_model, lora_model, role, rank, iter_name)
            
            # è·å–å½“å‰ç³»ç»Ÿæç¤º
            current_system_prompt = self._get_system_prompt()
            
            return f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼\n\nğŸ“‹ æ¨¡å‹ä¿¡æ¯ï¼š\nâ€¢ Baseæ¨¡å‹: {base_model}\n  {model_info['base_model']}\nâ€¢ Loraæ¨¡å‹: {lora_model if lora_model else 'æ— LoRA'}\n  {model_info['lora_model']}\nâ€¢ è§’è‰²: {ROLES[role]['name']}\n  {model_info['role']}\nâ€¢ Rank: {rank}\n  {model_info['rank']}\nâ€¢ Iter: {iter_name}\n  {model_info['iter']}", current_system_prompt
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
    
    # def chat(self, message, history, max_new_tokens=None, temperature=None, stop_words=None):
    #     """è¿›è¡ŒèŠå¤©å¯¹è¯ - ä½¿ç”¨Chatè¿›è¡Œå¯¹è¯ç”Ÿæˆ"""
    #     if chat.model is None or chat.tokenizer is None:
    #         return CHAT_CONFIG["default_response"]
    #
    #     try:
    #         # è·å–system prompt
    #         system_prompt = self._get_system_prompt()
    #
    #         # ä½¿ç”¨é»˜è®¤å€¼æˆ–ä¼ å…¥çš„å‚æ•°
    #         max_new_tokens = max_new_tokens or MODEL_CONFIG["max_new_tokens"]
    #         temperature = temperature or MODEL_CONFIG["temperature"]
    #
    #         # è®¾ç½®é»˜è®¤çš„stop words
    #         default_stop_words = ["</s>", "<eot_id>", "END", "Human:", "Assistant:", "User:", "Bot:"]
    #         if stop_words and stop_words.strip():
    #             stop_words_list = [word.strip() for word in stop_words.split(',') if word.strip()]
    #             stop_words_list.extend(default_stop_words)
    #         else:
    #             stop_words_list = default_stop_words
    #
    #         # å¦‚æœchatçš„å†å²ä¸ºç©ºä¸”éœ€è¦è®¾ç½®system promptï¼Œç›´æ¥æ·»åŠ åˆ°å†å²ä¸­
    #         if not chat.history and system_prompt:
    #             chat.history.append({"role": "system", "content": system_prompt})
    #
    #         # ä½¿ç”¨Chatè¿›è¡Œå¯¹è¯ç”Ÿæˆ
    #         response = chat.generate_response(
    #             message=[{"role": "user", "content": message}],
    #             max_new_tokens=max_new_tokens,
    #             temperature=temperature,
    #             stop_words=stop_words_list
    #         )
    #
    #         return response
    #
    #     except Exception as e:
    #         print(f"âŒ å¯¹è¯ç”Ÿæˆå¤±è´¥: {str(e)}")
    #         import traceback
    #         traceback.print_exc()
    #         return f"âŒ å¯¹è¯ç”Ÿæˆå¤±è´¥: {str(e)}"

    def chat_stream(self, message,  max_new_tokens, temperature):
        """è¿›è¡Œæµå¼èŠå¤©å¯¹è¯ - ä½¿ç”¨Chatè¿›è¡Œæµå¼å¯¹è¯ç”Ÿæˆ"""
        if chat.model is None or chat.tokenizer is None:
            yield CHAT_CONFIG["default_response"]
            return
        
        try:
            # è·å–system prompt
            system_prompt = self._get_system_prompt()
            
            # ä½¿ç”¨é»˜è®¤å€¼æˆ–ä¼ å…¥çš„å‚æ•°
            max_new_tokens = max_new_tokens or MODEL_CONFIG["max_new_tokens"]
            temperature = temperature or MODEL_CONFIG["temperature"]
            

            # å¦‚æœchatçš„å†å²ä¸ºç©ºä¸”éœ€è¦è®¾ç½®system promptï¼Œç›´æ¥æ·»åŠ åˆ°å†å²ä¸­
            if not chat.history and system_prompt:
                chat.history.append({"role": "system", "content": system_prompt})

            chat.history.append({"role": "user", "content": message})
            
            # ä½¿ç”¨Chatè¿›è¡Œæµå¼å¯¹è¯ç”Ÿæˆ
            for response in chat.generate_response_stream(
                max_new_tokens=max_new_tokens,
                temperature=temperature
            ):
                yield response
                
        except Exception as e:
            print(f"âŒ æµå¼å¯¹è¯ç”Ÿæˆå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            yield f"âŒ æµå¼å¯¹è¯ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def _get_model_type(self):
        """è·å–å½“å‰æ¨¡å‹ç±»å‹"""
        if self.current_base_model is None:
            return "llama"
        
        model_type = self.base_models[self.current_base_model]["type"]
        return model_type
    
    def _get_system_prompt(self):
        """æ ¹æ®è§’è‰²å’Œæ¨¡å‹ç±»å‹è·å–system prompt"""
        if self.current_role is None:
            return CHAT_CONFIG["system_prompt"]
        # LoRAä¸ºNoneæ—¶ï¼Œç›´æ¥è¿”å›è§’è‰²çš„ç³»ç»Ÿæ¶ˆæ¯
        if self.current_lora_model is None:
            return ROLE_SYSTEM_PROMPTS.get(self.current_role, CHAT_CONFIG["system_prompt"])
        # ft_ptç‰ˆæœ¬
        if self.current_lora_model.endswith("_ft_pt"):
            return ROLE_SYSTEM_PROMPTS.get(self.current_role, CHAT_CONFIG["system_prompt"])
        # å…¶ä»–æƒ…å†µè¿”å›ç©ºå­—ç¬¦ä¸²
        return ""
    


# åˆ›å»ºæ¨¡å‹ç®¡ç†å™¨å®ä¾‹
model_manager = ModelManager()

def update_lora_models(base_model):
    """æ›´æ–°loraæ¨¡å‹é€‰é¡¹"""
    if not base_model:
        return gr.Dropdown(choices=[], value=None)
    
    choices = model_manager.get_lora_models_for_base(base_model)
    return gr.Dropdown(choices=choices, value=choices[0][1] if choices else None)

def update_iters(base_model, lora_model, role, rank):
    """æ›´æ–°iteré€‰é¡¹"""
    if not all([base_model, role, rank]):
        return gr.Dropdown(choices=[], value=None)
    if lora_model is None:
        return gr.Dropdown(choices=["æ— "], value="æ— ")
    choices = model_manager.get_iters(base_model, lora_model, role, rank)
    return gr.Dropdown(choices=choices, value=choices[0] if choices else None)

# æ–°å¢update_rankså‡½æ•°
def update_ranks(lora_model):
    if lora_model is None:
        return gr.Dropdown(choices=["æ— "], value="æ— ", interactive=False)
    choices = model_manager.get_ranks()
    return gr.Dropdown(choices=choices, value=choices[0] if choices else None, interactive=True)

def load_model_wrapper(base_model, lora_model, role, rank, iter_name):
    """åŠ è½½æ¨¡å‹çš„åŒ…è£…å‡½æ•°"""
    result = model_manager.load_model(base_model, lora_model, role, rank, iter_name)
    if isinstance(result, tuple):
        return result
    else:
        return result, ""

def clear_model_wrapper():
    """æ¸…ç†æ¨¡å‹çš„åŒ…è£…å‡½æ•°"""
    try:
        chat.clear_model()
        return "âœ… æ¨¡å‹å·²æ¸…ç†ï¼ŒGPUå†…å­˜å·²é‡Šæ”¾", ""
    except Exception as e:
        return f"âŒ æ¨¡å‹æ¸…ç†å¤±è´¥: {str(e)}", ""

# def chat_wrapper(message, history):
#     """èŠå¤©çš„åŒ…è£…å‡½æ•°ï¼ˆå…¼å®¹Gradioæ ¼å¼ï¼‰"""
#     response = model_manager.chat(message, history)
#     # Gradio ChatbotæœŸæœ›å…ƒç»„æ ¼å¼ (user_message, assistant_message)
#     return history + [[message, response]]

def chat_stream_wrapper(message, history, max_new_tokens, temperature):
    """æµå¼èŠå¤©çš„åŒ…è£…å‡½æ•°"""
    # ç¡®ä¿å‚æ•°ç±»å‹æ­£ç¡®
    max_new_tokens = int(max_new_tokens)
    temperature = float(temperature)
    
    print(f"ğŸ”§ è°ƒè¯•ä¿¡æ¯ - max_new_tokens: {max_new_tokens}, temperature: {temperature}")
    
    # æµå¼ç”Ÿæˆå›å¤
    for response in model_manager.chat_stream(message, max_new_tokens, temperature):
        # Gradio ChatbotæœŸæœ›æ ¼å¼: [[user_msg, bot_msg], [user_msg2, bot_msg2], ...]
        # å°†ç”¨æˆ·æ¶ˆæ¯å’Œå½“å‰å›å¤ç»„åˆæˆæ–°çš„å¯¹è¯è½®æ¬¡
        current_conversation = [message, response]
        # å°†å†å²å¯¹è¯å’Œå½“å‰å¯¹è¯ç»„åˆ
        full_history = history + [current_conversation]
        yield full_history



# åˆ›å»ºGradioç•Œé¢
with gr.Blocks(title=WEB_CONFIG["title"], theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¥ NursingAI æ™ºèƒ½èŠå¤©ç³»ç»Ÿ")
    gr.Markdown("è¯·é€‰æ‹©baseæ¨¡å‹å’Œå¯¹åº”çš„loraæ¨¡å‹ï¼Œç„¶åå¼€å§‹èŠå¤©å¯¹è¯")
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### æ¨¡å‹é€‰æ‹©")
            
            # Baseæ¨¡å‹é€‰æ‹©
            base_model_dropdown = gr.Dropdown(
                choices=model_manager.get_available_models(),
                label="é€‰æ‹©Baseæ¨¡å‹",
                value=model_manager.get_available_models()[0] if model_manager.get_available_models() else None
            )
            
            # Loraæ¨¡å‹é€‰æ‹©
            initial_base_model = model_manager.get_available_models()[0] if model_manager.get_available_models() else None
            initial_lora_models = model_manager.get_lora_models_for_base(initial_base_model) if initial_base_model else []
            initial_lora_model = initial_lora_models[0][1] if initial_lora_models else None
            
            lora_model_dropdown = gr.Dropdown(
                choices=initial_lora_models,
                label="é€‰æ‹©Loraæ¨¡å‹",
                interactive=True,
                value=initial_lora_model
            )
            
            # è§’è‰²é€‰æ‹©
            role_dropdown = gr.Dropdown(
                choices=model_manager.get_roles(),
                label="é€‰æ‹©è§’è‰²",
                value=model_manager.get_roles()[0] if model_manager.get_roles() else None
            )
            
            # Ranké€‰æ‹©
            initial_rank_choices = ["æ— "] if initial_lora_model is None else model_manager.get_ranks()
            initial_rank_value = "æ— " if initial_lora_model is None else (initial_rank_choices[0] if initial_rank_choices else None)
            rank_dropdown = gr.Dropdown(
                choices=initial_rank_choices,
                label="é€‰æ‹©Lora Rank",
                value=initial_rank_value,
                interactive=(initial_lora_model is not None)
            )
            
            # Iteré€‰æ‹©
            initial_roles = model_manager.get_roles()
            initial_ranks = model_manager.get_ranks()
            initial_role = initial_roles[0] if initial_roles else None
            initial_rank = initial_ranks[0] if initial_ranks else None
            
            initial_iters = model_manager.get_iters(initial_base_model, initial_lora_model, initial_role, initial_rank) if all([initial_base_model, initial_lora_model, initial_role, initial_rank]) else []
            initial_iter = initial_iters[0] if initial_iters else None
            
            iter_dropdown = gr.Dropdown(
                choices=initial_iters,
                label="é€‰æ‹©Iter",
                interactive=True,
                value=initial_iter
            )
            
            # åŠ è½½æ¨¡å‹æŒ‰é’®
            load_btn = gr.Button("ğŸš€ åŠ è½½æ¨¡å‹", variant="primary")
            
            # æ¸…ç†æ¨¡å‹æŒ‰é’®
            clear_model_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç†æ¨¡å‹", variant="secondary")
            
            # åŠ è½½çŠ¶æ€æ˜¾ç¤º
            load_status = gr.Textbox(
                label="åŠ è½½çŠ¶æ€",
                interactive=False,
                lines=4
            )
        
        with gr.Column(scale=2):
            gr.Markdown("### èŠå¤©å¯¹è¯")
            
            # å½“å‰ç³»ç»Ÿæç¤ºæ˜¾ç¤º
            system_prompt_display = gr.Textbox(
                label="å½“å‰ç³»ç»Ÿæç¤º",
                interactive=False,
                lines=3,
                max_lines=5,
                placeholder="åŠ è½½æ¨¡å‹åæ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„ç³»ç»Ÿæç¤º..."
            )
            
            # èŠå¤©ç•Œé¢
            chatbot = gr.Chatbot(
                label="èŠå¤©è®°å½•",
                height=500,
                show_label=True
            )
            
            # é«˜çº§è®¾ç½®é¢æ¿
            initial_role = model_manager.get_roles()[0] if model_manager.get_roles() else None
            initial_max_tokens = ROLE_MAX_TOKENS.get(initial_role, 1024) if initial_role else 1024
            
            with gr.Accordion("ğŸ”§ é«˜çº§è®¾ç½®", open=False):
                max_new_tokens_slider = gr.Slider(
                    minimum=32, maximum=10000, value=initial_max_tokens, step=1, 
                    label="æœ€å¤§ç”ŸæˆTokenæ•°", info="ä¸“å®¶è§’è‰²é»˜è®¤10000ï¼Œå…¶ä»–è§’è‰²é»˜è®¤1024"
                )
                temperature_slider = gr.Slider(
                    minimum=0.01, maximum=2.0, value=0.1, step=0.01, 
                    label="æ¸©åº¦ç³»æ•°", info="æ§åˆ¶å›å¤çš„éšæœºæ€§ï¼Œå€¼è¶Šé«˜è¶Šéšæœº"
                )

            
            # è¾“å…¥æ¡†
            msg = gr.Textbox(
                label="è¾“å…¥æ¶ˆæ¯",
                placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                lines=2
            )
            
            # å‘é€æŒ‰é’®
            send_btn = gr.Button("ğŸ’¬ å‘é€", variant="primary")
            
            # æŒ‰é’®è¡Œ
            with gr.Row():
                # æ¸…é™¤æŒ‰é’®
                clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯")
                # å¤åˆ¶èŠå¤©è®°å½•æŒ‰é’®
                copy_btn = gr.Button("ğŸ“‹ å¤åˆ¶èŠå¤©è®°å½•")
    
    # ç»‘å®šäº‹ä»¶
    base_model_dropdown.change(
        fn=update_lora_models,
        inputs=[base_model_dropdown],
        outputs=[lora_model_dropdown]
    ).then(
        fn=update_iters,
        inputs=[base_model_dropdown, lora_model_dropdown, role_dropdown, rank_dropdown],
        outputs=[iter_dropdown]
    )
    
    lora_model_dropdown.change(
        fn=update_ranks,
        inputs=[lora_model_dropdown],
        outputs=[rank_dropdown]
    ).then(
        fn=update_iters,
        inputs=[base_model_dropdown, lora_model_dropdown, role_dropdown, rank_dropdown],
        outputs=[iter_dropdown]
    )
    
    role_dropdown.change(
        fn=update_iters,
        inputs=[base_model_dropdown, lora_model_dropdown, role_dropdown, rank_dropdown],
        outputs=[iter_dropdown]
    )
    
    rank_dropdown.change(
        fn=update_iters,
        inputs=[base_model_dropdown, lora_model_dropdown, role_dropdown, rank_dropdown],
        outputs=[iter_dropdown]
    )
    
    # è§’è‰²åˆ‡æ¢æ—¶æ›´æ–°æœ€å¤§tokenæ•°
    def update_max_tokens(role):
        return ROLE_MAX_TOKENS.get(role, 1024)
    
    role_dropdown.change(
        fn=update_max_tokens,
        inputs=[role_dropdown],
        outputs=[max_new_tokens_slider]
    )
    
    load_btn.click(
        fn=load_model_wrapper,
        inputs=[base_model_dropdown, lora_model_dropdown, role_dropdown, rank_dropdown, iter_dropdown],
        outputs=[load_status, system_prompt_display]
    )
    
    clear_model_btn.click(
        fn=clear_model_wrapper,
        inputs=None,
        outputs=[load_status, system_prompt_display]
    )

    send_btn.click(
        fn=chat_stream_wrapper,
        inputs=[msg, chatbot, max_new_tokens_slider, temperature_slider],
        outputs=[chatbot]
    ).then(
        lambda: "",
        None,
        [msg]
    )
    
    msg.submit(
        fn=chat_stream_wrapper,
        inputs=[msg, chatbot, max_new_tokens_slider, temperature_slider],
        outputs=[chatbot]
    ).then(
        lambda: "",
        None,
        [msg]
    )
    
    def clear_chat():
        """æ¸…ç©ºèŠå¤©å†å²å’Œç•Œé¢"""
        chat.clear_history()
        return []
    
    def copy_chat_history(chatbot_history):
        """å¤åˆ¶èŠå¤©è®°å½•åˆ°å‰ªè´´æ¿"""
        try:
            import pyperclip
        except ImportError:
            return "âŒ è¯·å…ˆå®‰è£…pyperclip: pip install pyperclip"
        
        if not chatbot_history:
            return "âŒ æ²¡æœ‰èŠå¤©è®°å½•å¯å¤åˆ¶"
        
        try:
            # æ ¹æ®å½“å‰æ¨¡å‹è§’è‰²è®¾ç½®å¯¹è¯è§’è‰²åç§°
            current_role = model_manager.current_role
            if current_role == "patient":
                role_names = ['æŠ¤å£«', 'æ‚£è€…']
            elif current_role == "expert":
                role_names = ['æŠ¤ç”Ÿ', 'ä¸“å®¶']
            elif current_role == "teacher":
                role_names = ['æŠ¤ç”Ÿ', 'æ•™å¸ˆ']
            else:
                role_names = ['ç”¨æˆ·', 'åŠ©æ‰‹']
            
            # å¤„ç†èŠå¤©è®°å½•æ ¼å¼
            formatted_history = []
            
            # å¤„ç†æ¯è½®å¯¹è¯
            for i, (user_msg, assistant_msg) in enumerate(chatbot_history, 1):
                formatted_history.append(f"{role_names[0]}: {user_msg}")
                formatted_history.append(f"{role_names[1]}: {assistant_msg}")
            
            # åˆå¹¶ä¸ºå­—ç¬¦ä¸²
            chat_text = "\n".join(formatted_history)
            
            # å¤åˆ¶åˆ°å‰ªè´´æ¿
            pyperclip.copy(chat_text)
            return f"âœ… èŠå¤©è®°å½•å·²å¤åˆ¶åˆ°å‰ªè´´æ¿ï¼\nå…±{len(chatbot_history)}è½®å¯¹è¯ï¼Œ{len(chat_text)}ä¸ªå­—ç¬¦"
            
        except Exception as e:
            return f"âŒ å¤åˆ¶å¤±è´¥: {str(e)}"
    
    clear_btn.click(
        fn=clear_chat,
        inputs=None,
        outputs=[chatbot]
    )
    
    copy_btn.click(
        fn=copy_chat_history,
        inputs=[chatbot],
        outputs=[load_status]  # ä½¿ç”¨load_statusæ˜¾ç¤ºå¤åˆ¶ç»“æœ
    )
    


if __name__ == "__main__":
    demo.launch(
        server_name=WEB_CONFIG["server_name"],
        server_port=WEB_CONFIG["server_port"],
        share=WEB_CONFIG["share"],
        debug=WEB_CONFIG["debug"],
        show_error=WEB_CONFIG["show_error"]
    ) 