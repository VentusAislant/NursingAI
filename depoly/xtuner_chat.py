#!/usr/bin/env python3
"""
ä½¿ç”¨XTunerè¿›è¡Œå¯¹è¯ç”Ÿæˆçš„æ¨¡å—
"""

import torch
from mmengine.config import ConfigDict

class XTunerChat:
    """ä½¿ç”¨XTunerè¿›è¡Œå¯¹è¯ç”Ÿæˆçš„ç±»"""
    
    def __init__(self):
        # å®šä¹‰XTuneré£æ ¼çš„å¯¹è¯æ¨¡æ¿
        self.prompt_templates = {
            "llama": ConfigDict(
                SYSTEM="<|start_header_id|>system<|end_header_id|>\n\n{system}<|eot_id|>",
                INSTRUCTION="<|start_header_id|>user<|end_header_id|>\n\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
                SUFFIX="<|eot_id|>",
                SUFFIX_AS_EOS=True,
                SEP="\n",
                STOP_WORDS=["<|eot_id|>"],
            ),
            "qwen": ConfigDict(
                SYSTEM="{system}\n\n",
                INSTRUCTION="REDACTED_SPECIAL_TOKEN{input}REDACTED_SPECIAL_TOKEN",
                SUFFIX="REDACTED_SPECIAL_TOKEN",
                SUFFIX_AS_EOS=True,
                STOP_WORDS=["REDACTED_SPECIAL_TOKEN"],
                SEP="\n",
            )
        }
    
    def build_prompt_text(self, text, system_prompt, template, n_turn=0, bot_name="BOT"):
        """æŒ‰ç…§XTunerçš„æ–¹å¼æ„å»ºpromptæ–‡æœ¬"""
        prompt_text = ""
        
        # å¤„ç†ç³»ç»Ÿæç¤ºï¼ˆåªåœ¨ç¬¬ä¸€è½®æ·»åŠ ï¼‰
        if "SYSTEM" in template and n_turn == 0 and system_prompt:
            prompt_text += template.SYSTEM.format(
                system=system_prompt, round=n_turn + 1, bot_name=bot_name
            )
        
        # æ·»åŠ æŒ‡ä»¤éƒ¨åˆ†
        prompt_text += template.INSTRUCTION.format(
            input=text, round=n_turn + 1, bot_name=bot_name
        )
        
        return prompt_text
    
    def build_conversation(self, message, history, system_prompt, model_type="llama"):
        """æŒ‰ç…§XTuneræ–¹å¼æ„å»ºå®Œæ•´å¯¹è¯"""
        template = self.prompt_templates.get(model_type, self.prompt_templates["llama"])
        
        # æ„å»ºå®Œæ•´çš„å¯¹è¯æ–‡æœ¬
        inputs = ""
        
        # å¤„ç†å†å²å¯¹è¯
        for i, (human, assistant) in enumerate(history):
            # æ„å»ºç”¨æˆ·è¾“å…¥éƒ¨åˆ†
            prompt_text = self.build_prompt_text(human, "", template, i, "BOT")
            inputs += prompt_text
            
            # æ·»åŠ åŠ©æ‰‹å›å¤
            inputs += assistant
            
            # æ·»åŠ åˆ†éš”ç¬¦
            if template.get("SEP"):
                inputs += template.SEP
        
        # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
        current_prompt = self.build_prompt_text(message, system_prompt, template, len(history), "BOT")
        inputs += current_prompt
        
        return inputs, template
    
    def generate_response(self, model, tokenizer, message, history, system_prompt, model_type="llama", **kwargs):
        """ä½¿ç”¨XTuneré£æ ¼ç”Ÿæˆå›å¤"""
        # æ„å»ºå¯¹è¯
        inputs, template = self.build_conversation(message, history, system_prompt, model_type)
        
        # è¯¦ç»†æ‰“å°è°ƒè¯•ä¿¡æ¯
        print("\n" + "="*80)
        print("ğŸ” XTunerè¯¦ç»†è°ƒè¯•ä¿¡æ¯")
        print("="*80)
        print(f"ğŸ“‹ æ¨¡å‹ç±»å‹: {model_type}")
        print(f"ğŸ“ ç³»ç»Ÿæç¤º: {system_prompt}")
        print(f"ğŸ’¬ ç”¨æˆ·è¾“å…¥: {message}")
        print(f"ğŸ“š å¯¹è¯å†å²: {history}")
        print(f"ğŸ”§ ä½¿ç”¨çš„æ¨¡æ¿: {template}")
        print("\n" + "-"*80)
        print("ğŸ“„ å®Œæ•´å¯¹è¯æ¨¡æ¿:")
        print("-"*80)
        print(inputs)
        print("-"*80)
        
        # ç”Ÿæˆå›å¤
        print(f"\nğŸ¤– å¼€å§‹æ¨ç†ï¼Œä½¿ç”¨XTuneré£æ ¼")
        
        # ç¼–ç è¾“å…¥
        if len(history) == 0:
            # ç¬¬ä¸€è½®å¯¹è¯
            ids = tokenizer.encode(inputs, return_tensors="pt")
        else:
            # åç»­å¯¹è¯ï¼Œä¸æ·»åŠ ç‰¹æ®Štoken
            ids = tokenizer.encode(inputs, return_tensors="pt", add_special_tokens=False)
        
        # ç¡®ä¿è¾“å…¥æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if torch.cuda.is_available():
            ids = ids.cuda()
        else:
            ids = ids.cpu()
        print(f"ğŸ“Š è¾“å…¥tokensé•¿åº¦: {ids.shape[1]}")
        
        # è®¾ç½®åœæ­¢æ¡ä»¶
        stop_words = template.STOP_WORDS
        print(f"ğŸ›‘ åœæ­¢è¯: {stop_words}")
        
        # åˆ›å»ºåœæ­¢æ¡ä»¶
        def create_stopping_criteria(tokenizer, stop_words):
            from transformers import StoppingCriteria, StoppingCriteriaList
            
            class StopWordsCriteria(StoppingCriteria):
                def __init__(self, tokenizer, stop_words):
                    self.tokenizer = tokenizer
                    self.stop_words = stop_words
                    self.stop_word_ids = []
                    for stop_word in stop_words:
                        stop_word_ids = tokenizer.encode(stop_word, add_special_tokens=False)
                        self.stop_word_ids.append(stop_word_ids)
                
                def __call__(self, input_ids, scores, **kwargs):
                    for stop_word_ids in self.stop_word_ids:
                        if len(input_ids[0]) >= len(stop_word_ids):
                            if input_ids[0][-len(stop_word_ids):].tolist() == stop_word_ids:
                                return True
                    return False
            
            return StoppingCriteriaList([StopWordsCriteria(tokenizer, stop_words)])
        
        stopping_criteria = create_stopping_criteria(tokenizer, stop_words)
        
        # è®¾ç½®ç”Ÿæˆå‚æ•°
        generation_kwargs = {
            "max_new_tokens": kwargs.get("max_new_tokens", 512),
            "temperature": kwargs.get("temperature", 0.1),
            "do_sample": kwargs.get("do_sample", True),
            "pad_token_id": tokenizer.eos_token_id,
            "eos_token_id": tokenizer.eos_token_id,
            "stopping_criteria": stopping_criteria,
        }
        
        with torch.no_grad():
            outputs = model.generate(
                inputs=ids,
                **generation_kwargs
            )
        
        # è§£ç è¾“å‡ºï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
        raw_response = tokenizer.decode(outputs[0][ids.shape[1]:], skip_special_tokens=True)
        
        print(f"\nğŸ“¤ åŸå§‹æ¨¡å‹è¾“å‡º:")
        print("-"*80)
        print(raw_response)
        print("-"*80)
        
        # æ¸…ç†å“åº”ä¸­çš„åœæ­¢è¯ï¼ˆä»¥é˜²ä¸‡ä¸€ï¼‰
        response = raw_response
        for stop_word in template.STOP_WORDS:
            if stop_word in response:
                response = response.replace(stop_word, "")
                print(f"ğŸ§¹ æ¸…ç†åœæ­¢è¯ '{stop_word}'")
        
        final_response = response.strip()
        print(f"\nâœ… æœ€ç»ˆå“åº”: {final_response}")
        print("="*80 + "\n")
        
        return final_response

    def generate_response_stream(self, model, tokenizer, message, history, system_prompt, model_type="llama", **kwargs):
        """ä½¿ç”¨XTuneré£æ ¼ç”Ÿæˆæµå¼å›å¤"""
        # æ„å»ºå¯¹è¯
        inputs, template = self.build_conversation(message, history, system_prompt, model_type)
        
        # ç®€åŒ–è°ƒè¯•ä¿¡æ¯ï¼Œæé«˜æ€§èƒ½
        print(f"\nğŸ¤– å¼€å§‹æµå¼æ¨ç† - æ¨¡å‹ç±»å‹: {model_type}")
        
        # ç¼–ç è¾“å…¥
        if len(history) == 0:
            # ç¬¬ä¸€è½®å¯¹è¯
            ids = tokenizer.encode(inputs, return_tensors="pt")
        else:
            # åç»­å¯¹è¯ï¼Œä¸æ·»åŠ ç‰¹æ®Štoken
            ids = tokenizer.encode(inputs, return_tensors="pt", add_special_tokens=False)
        
        # ç¡®ä¿è¾“å…¥æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if torch.cuda.is_available():
            ids = ids.cuda()
        else:
            ids = ids.cpu()
        
        # è®¾ç½®åœæ­¢æ¡ä»¶
        stop_words = template.STOP_WORDS
        
        # åˆ›å»ºåœæ­¢æ¡ä»¶
        def create_stopping_criteria(tokenizer, stop_words):
            from transformers import StoppingCriteria, StoppingCriteriaList
            
            class StopWordsCriteria(StoppingCriteria):
                def __init__(self, tokenizer, stop_words):
                    self.tokenizer = tokenizer
                    self.stop_words = stop_words
                    self.stop_word_ids = []
                    for stop_word in stop_words:
                        stop_word_ids = tokenizer.encode(stop_word, add_special_tokens=False)
                        self.stop_word_ids.append(stop_word_ids)
                
                def __call__(self, input_ids, scores, **kwargs):
                    for stop_word_ids in self.stop_word_ids:
                        if len(input_ids[0]) >= len(stop_word_ids):
                            if input_ids[0][-len(stop_word_ids):].tolist() == stop_word_ids:
                                return True
                    return False
            
            return StoppingCriteriaList([StopWordsCriteria(tokenizer, stop_words)])
        
        stopping_criteria = create_stopping_criteria(tokenizer, stop_words)
        
        # ä¼˜åŒ–ç”Ÿæˆå‚æ•°ï¼Œæé«˜æµç•…åº¦
        generation_kwargs = {
            "max_new_tokens": kwargs.get("max_new_tokens", 512),
            "temperature": kwargs.get("temperature", 0.1),
            "do_sample": kwargs.get("do_sample", True),
            "pad_token_id": tokenizer.eos_token_id,
            "eos_token_id": tokenizer.eos_token_id,
            "stopping_criteria": stopping_criteria,
            "streamer": None,  # å°†åœ¨ä¸‹é¢è®¾ç½®
            "use_cache": True,  # å¯ç”¨ç¼“å­˜æé«˜æ€§èƒ½
        }
        
        # åˆ›å»ºæµå¼ç”Ÿæˆå™¨
        from transformers import TextIteratorStreamer
        import threading
        
        streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True, timeout=10)
        generation_kwargs["streamer"] = streamer
        
        # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œç”Ÿæˆ
        generation_thread = threading.Thread(
            target=model.generate,
            kwargs={"inputs": ids, **generation_kwargs}
        )
        generation_thread.start()
        
        # æµå¼è¾“å‡º
        accumulated_text = ""
        for new_text in streamer:
            if new_text:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«åœæ­¢è¯
                should_stop = False
                cleaned_text = new_text
                
                for stop_word in template.STOP_WORDS:
                    if stop_word in new_text:
                        cleaned_text = new_text.replace(stop_word, "")
                        should_stop = True
                        break
                
                accumulated_text += cleaned_text
                # åªè¿”å›æ–°ç”Ÿæˆçš„å†…å®¹ï¼Œä¸åŒ…å«ç”¨æˆ·è¾“å…¥
                yield accumulated_text.strip()
                
                # å¦‚æœæ£€æµ‹åˆ°åœæ­¢è¯ï¼Œæå‰ç»“æŸ
                if should_stop:
                    break
        
        # ç­‰å¾…ç”Ÿæˆçº¿ç¨‹å®Œæˆ
        generation_thread.join()
        
        final_response = accumulated_text.strip()
        print(f"âœ… æµå¼ç”Ÿæˆå®Œæˆ")
        
        return final_response

# åˆ›å»ºå…¨å±€å®ä¾‹
xtuner_chat = XTunerChat() 